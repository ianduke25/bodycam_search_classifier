{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f14287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, make_scorer, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir('/Users/ianduke/Desktop/for_tarak')\n",
    "\n",
    "from text_analysis_toolkit import TextAnalysisToolkit\n",
    "\n",
    "# Create an instance of the toolkit\n",
    "toolkit = TextAnalysisToolkit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e788f88",
   "metadata": {},
   "source": [
    "## **S1**: Save transcript text to a dataframe. Clean + apply embedding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6203c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all transcripts to one csv\n",
    "directory_path = # ADD PATH\n",
    "output_path_csv = #ADD PATH\n",
    "\n",
    "filepaths, contents = toolkit.collect_txt_files_data(directory_path)\n",
    "toolkit.save_data_to_csv(filepaths, contents, output_path_csv)\n",
    "\n",
    "data = pd.read_csv(output_path_csv)\n",
    "\n",
    "labels = pd.read_csv(# ADD PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a50f98c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract just the filename from filepath\n",
    "data['filename'] = data['filepath'].str.split('/').str[-1]\n",
    "\n",
    "# Only include labeled files\n",
    "data = data[data['filename'].isin(labels['File'])]\n",
    "\n",
    "# Reset index after dropping unlabeled files\n",
    "data = data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "434f2ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the transcripts\n",
    "data['transcript'] = data['transcript'].apply(toolkit.clean_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3893c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great! now lets encode our all sentences in our fancy new transcript dataframe\n",
    "data_encoded_sbert= toolkit.encode_sentences_sbert(data, 'transcript')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2f382ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean transcription and embedding columns\n",
    "data_encoded_sbert['transcript'] = data_encoded_sbert['transcript'].apply(lambda x: str(x) if pd.notnull(x) else \"\")\n",
    "data_encoded_sbert['embeddings'] = data_encoded_sbert['embeddings'].apply(lambda x: np.array(ast.literal_eval(x)) if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a16f0d4",
   "metadata": {},
   "source": [
    "### **S2**: Create one dataframe containing merged video data and search labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02d83d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean merged_df\n",
    "merged_df = data_encoded_sbert.merge(labels, left_on='filename', right_on='File', how='inner')\n",
    "merged_df = merged_df.loc[:, ~merged_df.columns.str.startswith('Unnamed')]\n",
    "merged_df = merged_df.loc[:, ~merged_df.columns.str.endswith('_text')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d1ad3c",
   "metadata": {},
   "source": [
    "### **S3**: Define reference sentences for cosine similarity features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49c20af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = toolkit.related_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de7fa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Embedding Model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Run custom model to classify transcripts\n",
    "for i, sentence in enumerate(tqdm(all_sentences, desc=\"Processing sentences\")):\n",
    "    target_sentence = sentence\n",
    "    data_final = toolkit.find_closest_sentences_sbert(merged_df, 'transcript','embeddings', target_sentence, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e6e67",
   "metadata": {},
   "source": [
    "### **S4**: Engineer additional features not based on cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d252d32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function along the rows (axis=1) and create new columns in the DataFrame\n",
    "data_final[['num_questions', 'num_sentences']] = data_final.apply(toolkit.count_questions_and_sentences, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a60898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate aggregated embeddings\n",
    "data_final['mean_embedding'] = ''\n",
    "data_final['sum_embedding'] = ''\n",
    "data_final['mean_embeddings_final_5'] = ''\n",
    "data_final['mean_embeddings_first_5'] = ''\n",
    "for i in range(len(data_final)):\n",
    "    data_final['mean_embedding'][i] = np.mean(data_final['embeddings'][i])\n",
    "    data_final['sum_embedding'][i] = np.sum(data_final['embeddings'][i])\n",
    "    data_final['mean_embeddings_final_5'][i] = np.mean(data_final['embeddings'][i][-5:])\n",
    "    data_final['mean_embeddings_first_5'][i] = np.mean(data_final['embeddings'][i][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ba182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add counts for keywords\n",
    "keywords = ['confiscated', 'confiscate', 'search', 'marijuana', 'consent', 'weed', 'look', 'open', 'trunk']\n",
    "data_final = toolkit.count_keywords_in_transcripts_case_insensitive(data_final, keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adca592",
   "metadata": {},
   "source": [
    "### **S5**: Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25c5e7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.fillna(0, inplace=True)\n",
    "\n",
    "X = data_final.drop(['Search?', 'transcript', 'embeddings', 'File', 'filename', 'sentences'], axis = 1)\n",
    "X = X.drop(columns=[col for col in X.columns if col.endswith('_similarity_text')])\n",
    "\n",
    "\n",
    "y = data_final['Search?']\n",
    "\n",
    "X_train_files, X_test_files, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state =13)\n",
    "\n",
    "X_train = X_train_files.drop(['filepath'], axis = 1)\n",
    "X_test = X_test_files.drop(['filepath'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c4d75bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's rescale our data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform it\n",
    "x_train_sc_array = scaler.fit_transform(X_train)\n",
    "x_test_sc_array = scaler.transform(X_test)\n",
    "\n",
    "# Convert the scaled array back to a DataFrame\n",
    "x_train_sc = pd.DataFrame(x_train_sc_array, columns=X_train.columns)\n",
    "x_test_sc = pd.DataFrame(x_test_sc_array, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0c0c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop raw text from merged df data\n",
    "merged_df = merged_df.drop('transcript', axis = 1)\n",
    "\n",
    "# Reset y_train index\n",
    "y_train.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c85bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_info_L1 = toolkit.fit_logistic_and_find_best_score(x_train_sc, y_train)\n",
    "best_model_info_L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e257bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify only non-zero coefficients\n",
    "non_zero_coefficients = []\n",
    "for element in best_model_info_L1['feature_coefficients']:\n",
    "    if element[1] != 0:\n",
    "        non_zero_coefficients.append(element[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab33d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the original X_train DataFrame to keep only significant features\n",
    "X_train_filtered = X_train[non_zero_coefficients]\n",
    "\n",
    "# Filter the scaled x_train_sc DataFrame similarly\n",
    "x_train_sc_filtered = x_train_sc[non_zero_coefficients]\n",
    "\n",
    "x_test_sc_filtered = x_test_sc[non_zero_coefficients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e253fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_info_L1 = toolkit.fit_logistic_and_find_best_score(x_train_sc_filtered, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691bae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = best_model_info_L1['best_alpha']\n",
    "model = LogisticRegression(penalty='l1', C=1/best_alpha, solver='liblinear', random_state=25, class_weight='balanced')\n",
    "model.fit(x_train_sc_filtered, y_train)\n",
    "\n",
    "# Get probability estimates for the test data\n",
    "probabilities = model.predict_proba(x_train_sc_filtered)\n",
    "\n",
    "# Apply the custom threshold of 0.5 to the positive class's probability estimates\n",
    "custom_threshold = 0.4\n",
    "predictions_custom_threshold = (probabilities[:, 1] > custom_threshold).astype(int)\n",
    "\n",
    "# Now you can evaluate your model using these custom predictions\n",
    "test_accuracy_custom = accuracy_score(y_train, predictions_custom_threshold)\n",
    "test_precision_custom = precision_score(y_train, predictions_custom_threshold, zero_division=0)\n",
    "test_recall_custom = recall_score(y_train, predictions_custom_threshold, zero_division=0)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Train Accuracy with custom threshold: {test_accuracy_custom}')\n",
    "print(f'Train Precision with custom threshold: {test_precision_custom}')\n",
    "print(f'Train Recall with custom threshold: {test_recall_custom}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f96fccc",
   "metadata": {},
   "source": [
    "### **S6**: After selecting final model, run on unseen test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdabd772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probability estimates for the test data\n",
    "probabilities = model.predict_proba(x_test_sc_filtered)\n",
    "\n",
    "# Apply the custom threshold of 0.5 to the positive class's probability estimates\n",
    "custom_threshold = 0.4\n",
    "predictions_custom_threshold = (probabilities[:, 1] > custom_threshold).astype(int)\n",
    "\n",
    "# Now you can evaluate your model using these custom predictions\n",
    "test_accuracy_custom = accuracy_score(y_test, predictions_custom_threshold)\n",
    "test_precision_custom = precision_score(y_test, predictions_custom_threshold, zero_division=0)\n",
    "test_recall_custom = recall_score(y_test, predictions_custom_threshold, zero_division=0)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Test Accuracy with custom threshold: {test_accuracy_custom}')\n",
    "print(f'Test Precision with custom threshold: {test_precision_custom}')\n",
    "print(f'Test Recall with custom threshold: {test_recall_custom}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08229e3",
   "metadata": {},
   "source": [
    "### **S7**: Export Trained Model to Pickle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5e679759",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('search_classifier_09_01.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACLU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
